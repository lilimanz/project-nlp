{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unidecode\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cuando conocí a Janice en 2013 , una familia n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hwang habló en Sur de este año por Southwest M...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Usted podría pensar Katy Perry y Robert Pattin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cualquiera que haya volado los cielos del crea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bueno , este cantante tendrá un LARGO tiempo p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  Cuando conocí a Janice en 2013 , una familia n...      1\n",
       "1  Hwang habló en Sur de este año por Southwest M...      0\n",
       "2  Usted podría pensar Katy Perry y Robert Pattin...      1\n",
       "3  Cualquiera que haya volado los cielos del crea...      1\n",
       "4  Bueno , este cantante tendrá un LARGO tiempo p...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from a text file\n",
    "def load_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            label, sentence = line.strip().split('\\t', 1)  # Split on the first tab character\n",
    "            data.append([sentence, int(label)])\n",
    "    return pd.DataFrame(data, columns=['sentence', 'label'])\n",
    "\n",
    "# file_path\n",
    "file_path = 'TRAINING_DATA.txt'\n",
    "\n",
    "#assign the data into pandas DF\n",
    "data = load_data(file_path)\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another method - Lemmatize and BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include lemmatize and BoW\n",
    "def preprocess_text_lemmatize(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove accents\n",
    "    text = unidecode.unidecode(text)\n",
    "    # Normalize numbers: Replace digits with a special token, e.g., \"NUM\"\n",
    "    text = re.sub(r'\\d+', 'NUM', text)\n",
    "    #removing special characters \n",
    "    text = re.sub(r'[^\\w\\sáéíóúüñ¿¡0-9]', '', text) \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    # Define stop words for Spanish\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    # Remove stop words from the tokenized words\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # Join the words back into a single string with spaces\n",
    "    return ' '.join(words)\n",
    "\n",
    "data['cleaned_setenced']= data['sentence'].apply(preprocess_text_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_setenced</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cuando conocí a Janice en 2013 , una familia n...</td>\n",
       "      <td>1</td>\n",
       "      <td>conoci janice 2013 familia necesitaba 600 punt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hwang habló en Sur de este año por Southwest M...</td>\n",
       "      <td>0</td>\n",
       "      <td>hwang hablo sur ano southwest music and medium...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Usted podría pensar Katy Perry y Robert Pattin...</td>\n",
       "      <td>1</td>\n",
       "      <td>usted podria pensar katy perry robert pattinso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cualquiera que haya volado los cielos del crea...</td>\n",
       "      <td>1</td>\n",
       "      <td>cualquiera volado cielos creador escuchado act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bueno , este cantante tendrá un LARGO tiempo p...</td>\n",
       "      <td>1</td>\n",
       "      <td>bueno cantante tendra largo tiempo sentir aun ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  \\\n",
       "0  Cuando conocí a Janice en 2013 , una familia n...      1   \n",
       "1  Hwang habló en Sur de este año por Southwest M...      0   \n",
       "2  Usted podría pensar Katy Perry y Robert Pattin...      1   \n",
       "3  Cualquiera que haya volado los cielos del crea...      1   \n",
       "4  Bueno , este cantante tendrá un LARGO tiempo p...      1   \n",
       "\n",
       "                                    cleaned_setenced  \n",
       "0  conoci janice 2013 familia necesitaba 600 punt...  \n",
       "1  hwang hablo sur ano southwest music and medium...  \n",
       "2  usted podria pensar katy perry robert pattinso...  \n",
       "3  cualquiera volado cielos creador escuchado act...  \n",
       "4  bueno cantante tendra largo tiempo sentir aun ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3559843400447427\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.35      0.35      1751\n",
      "           1       0.37      0.36      0.36      1825\n",
      "\n",
      "    accuracy                           0.36      3576\n",
      "   macro avg       0.36      0.36      0.36      3576\n",
      "weighted avg       0.36      0.36      0.36      3576\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anama\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "def count_vectorize(data, text_column):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(data[text_column])\n",
    "    return X, vectorizer\n",
    "\n",
    "\n",
    "# Logistic Regression Classification function\n",
    "def logistic_regression_classification_2(X, y):\n",
    "    # Train-Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train the classifier\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Evaluate the classifier\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, report, classifier\n",
    "\n",
    "# Main function to tie everything together\n",
    "def main_tfidf_log_reg_2(data, text_column):\n",
    "    # TF-IDF Vectorization\n",
    "    X, vectorizer = count_vectorize(data, text_column)\n",
    "    y = data['label']\n",
    "    \n",
    "    # Classification\n",
    "    accuracy, report, classifier = logistic_regression_classification_2(X, y)\n",
    "    \n",
    "    # Output results\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(report)\n",
    "    \n",
    "    return vectorizer, classifier\n",
    "\n",
    "vectorizer, classifier = main_tfidf_log_reg_2(data,'cleaned_setenced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying gradient boost with count vectorize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5109060402684564\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.13      0.21      1751\n",
      "           1       0.51      0.88      0.65      1825\n",
      "\n",
      "    accuracy                           0.51      3576\n",
      "   macro avg       0.51      0.50      0.43      3576\n",
      "weighted avg       0.51      0.51      0.43      3576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def gradient_boosting_classification_count(X, y):\n",
    "    # Train-Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train the classifier\n",
    "    classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Evaluate the classifier\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, report, classifier\n",
    "\n",
    "# Main function to tie everything together\n",
    "def main_gradient_boosting_count(data, text_column):\n",
    "    # Count Vectorization\n",
    "    X, vectorizer = count_vectorize(data, text_column)\n",
    "    y = data['label']\n",
    "    \n",
    "    # Classification\n",
    "    accuracy, report, classifier = gradient_boosting_classification_count(X, y)\n",
    "    \n",
    "    # Output results\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(report)\n",
    "    \n",
    "    return vectorizer, classifier \n",
    "\n",
    "# Call the main function with gradient boosting\n",
    "vectorizer, classifier = main_gradient_boosting_count(data, 'cleaned_setenced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Best hyperparameters: {'classifier__learning_rate': 0.01, 'classifier__n_estimators': 200, 'count_vectorizer__max_features': 3000}\n",
      "Best cross-validation score: 0.5152430343705511\n",
      "Test score of the best model: 0.5262863534675615\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#data definition\n",
    "X= data['cleaned_setenced']\n",
    "y= data['label']\n",
    "\n",
    "#splitting the data \n",
    "X_train, X_test, y_train, y_test = train_test_split(data['cleaned_setenced'], data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define parameter grids for both CountVectorizer and GradientBoostingClassifier\n",
    "param_grid = {\n",
    "    'count_vectorizer__max_features': [1000, 2000, 3000],  # Example parameter grid for CountVectorizer\n",
    "    'classifier__n_estimators': [50, 100, 200],            # Example parameter grid for GradientBoostingClassifier\n",
    "    'classifier__learning_rate': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('count_vectorizer', CountVectorizer()),\n",
    "    ('classifier', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X,y)\n",
    "\n",
    "# Get best hyperparameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Print best hyperparameters and best score\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "print(\"Best cross-validation score:\", best_score)\n",
    "\n",
    "# Evaluate the best model on a holdout test set\n",
    "best_model = grid_search.best_estimator_\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "print(\"Test score of the best model:\", test_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
