{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unidecode\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cuando conocí a Janice en 2013 , una familia n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hwang habló en Sur de este año por Southwest M...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Usted podría pensar Katy Perry y Robert Pattin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cualquiera que haya volado los cielos del crea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bueno , este cantante tendrá un LARGO tiempo p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  Cuando conocí a Janice en 2013 , una familia n...      1\n",
       "1  Hwang habló en Sur de este año por Southwest M...      0\n",
       "2  Usted podría pensar Katy Perry y Robert Pattin...      1\n",
       "3  Cualquiera que haya volado los cielos del crea...      1\n",
       "4  Bueno , este cantante tendrá un LARGO tiempo p...      1"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from a text file\n",
    "def load_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            label, sentence = line.strip().split('\\t', 1)  # Split on the first tab character\n",
    "            data.append([sentence, int(label)])\n",
    "    return pd.DataFrame(data, columns=['sentence', 'label'])\n",
    "\n",
    "# file_path\n",
    "file_path = 'TRAINING_DATA.txt'\n",
    "\n",
    "#assign the data into pandas DF\n",
    "data = load_data(file_path)\n",
    "\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17877, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    8939\n",
       "1    8938\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anama\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anama\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk resources \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning, tokenization and stop words \n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove accents\n",
    "    text = unidecode.unidecode(text)\n",
    "    # Normalize numbers: Replace digits with a special token, e.g., \"NUM\"\n",
    "    text = re.sub(r'\\d+', 'NUM', text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    #special characters \n",
    "    text = re.sub(r'[^\\w\\sáéíóúüñ¿¡]', '', text) \n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    # Define stop words for Spanish\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    # Remove stop words from the tokenized words\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Join the words back into a single string with spaces-\n",
    "    return ' '.join(words)\n",
    "\n",
    "data['cleaned_sentenced'] = data['sentence'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_sentenced</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cuando conocí a Janice en 2013 , una familia n...</td>\n",
       "      <td>1</td>\n",
       "      <td>conoci janice 2013 familia necesitaba 600 punt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hwang habló en Sur de este año por Southwest M...</td>\n",
       "      <td>0</td>\n",
       "      <td>hwang hablo sur ano southwest music and media ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Usted podría pensar Katy Perry y Robert Pattin...</td>\n",
       "      <td>1</td>\n",
       "      <td>usted podria pensar katy perry robert pattinso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cualquiera que haya volado los cielos del crea...</td>\n",
       "      <td>1</td>\n",
       "      <td>cualquiera volado cielos creador escuchado act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bueno , este cantante tendrá un LARGO tiempo p...</td>\n",
       "      <td>1</td>\n",
       "      <td>bueno cantante tendra largo tiempo sentir aun ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label  \\\n",
       "0  Cuando conocí a Janice en 2013 , una familia n...      1   \n",
       "1  Hwang habló en Sur de este año por Southwest M...      0   \n",
       "2  Usted podría pensar Katy Perry y Robert Pattin...      1   \n",
       "3  Cualquiera que haya volado los cielos del crea...      1   \n",
       "4  Bueno , este cantante tendrá un LARGO tiempo p...      1   \n",
       "\n",
       "                                   cleaned_sentenced  \n",
       "0  conoci janice 2013 familia necesitaba 600 punt...  \n",
       "1  hwang hablo sur ano southwest music and media ...  \n",
       "2  usted podria pensar katy perry robert pattinso...  \n",
       "3  cualquiera volado cielos creador escuchado act...  \n",
       "4  bueno cantante tendra largo tiempo sentir aun ...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Mapping: [0 1]\n",
      "Accuracy: 0.3615771812080537\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.36      0.36      1774\n",
      "           1       0.37      0.37      0.37      1802\n",
      "\n",
      "    accuracy                           0.36      3576\n",
      "   macro avg       0.36      0.36      0.36      3576\n",
      "weighted avg       0.36      0.36      0.36      3576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Vectorization function and logistic_regression\n",
    "def tfidf_vectorize(data, text_column):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(data[text_column])\n",
    "    return X, vectorizer\n",
    "\n",
    "# Logistic Regression Classification function\n",
    "def logistic_regression_classification(X, y):\n",
    "    # Train-Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,) #random_state=42)\n",
    "    \n",
    "    # Train the classifier\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Evaluate the classifier\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, report, classifier\n",
    "\n",
    "# Main function to tie everything together\n",
    "def main_tfidf_log_reg(data, text_column):\n",
    "    # TF-IDF Vectorization\n",
    "    X, vectorizer = tfidf_vectorize(data, text_column)\n",
    "    y = data['label']\n",
    "\n",
    "    # Classification\n",
    "    accuracy, report, classifier = logistic_regression_classification(X, y)\n",
    "    \n",
    "\n",
    "    # Print class mapping\n",
    "    class_mapping = classifier.classes_\n",
    "    print(f'Class Mapping: {class_mapping}')\n",
    "    \n",
    "    # Output results\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(report)\n",
    "    \n",
    "    return vectorizer, classifier\n",
    "\n",
    "\n",
    "vectorizer, classifier = main_tfidf_log_reg(data, 'cleaned_sentenced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grandient boost algo with td-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5050335570469798\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.14      0.22      1751\n",
      "           1       0.51      0.86      0.64      1825\n",
      "\n",
      "    accuracy                           0.51      3576\n",
      "   macro avg       0.50      0.50      0.43      3576\n",
      "weighted avg       0.50      0.51      0.43      3576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Gradient boosting with td-idf\n",
    "def gradient_boosting_classification(X, y):\n",
    "    # Train-Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train the classifier\n",
    "    classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Evaluate the classifier\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, report, classifier\n",
    "\n",
    "# Main function to tie everything together\n",
    "def main_gradient_boosting(data, text_column):\n",
    "    # Count Vectorization\n",
    "    X, vectorizer = tfidf_vectorize(data, text_column)\n",
    "    y = data['label']\n",
    "    \n",
    "    # Classification\n",
    "    accuracy, report, classifier = gradient_boosting_classification(X, y)\n",
    "    \n",
    "    # Output results\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(report)\n",
    "    \n",
    "    return vectorizer, classifier \n",
    "\n",
    "# Call the main function with gradient boosting\n",
    "vectorizer, classifier = main_gradient_boosting(data, 'cleaned_sentenced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = model.classes_\n",
    "print(class_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross- Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anama\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 128 candidates, totalling 640 fits\n",
      "Best hyperparameters: {'classifier__learning_rate': 0.01, 'classifier__n_estimators': 100, 'tfidf__max_df': 0.8, 'tfidf__max_features': 1000, 'tfidf__min_df': 0.01, 'tfidf__ngram_range': (1, 1), 'tfidf__stop_words': None}\n",
      "Best cross-validation score: 0.5024121352258367\n",
      "Test score of the best model: 0.5195749440715883\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.11      0.19      1751\n",
      "           1       0.52      0.91      0.66      1825\n",
      "\n",
      "    accuracy                           0.52      3576\n",
      "   macro avg       0.53      0.51      0.42      3576\n",
      "weighted avg       0.53      0.52      0.43      3576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure stop words are downloaded\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load your data into a DataFrame named `data`\n",
    "# data = pd.read_csv('your_data.csv')  # Example loading\n",
    "\n",
    "# Data definition\n",
    "X = data['cleaned_sentenced']\n",
    "y = data['label']\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define Spanish stop words\n",
    "spanish_stop_words = stopwords.words('spanish')\n",
    "\n",
    "# Define parameter grids for both TfidfVectorizer and GradientBoostingClassifier\n",
    "param_grid = {\n",
    "    'tfidf__max_features': [1000, 2000],  # Reduce number of combinations\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],  # Reduce number of combinations\n",
    "    'tfidf__max_df': [0.8, 0.9],  # Reduce number of combinations\n",
    "    'tfidf__min_df': [0.01, 0.05],  # Reduce number of combinations\n",
    "    'tfidf__stop_words': [None, spanish_stop_words],  # Use list of Spanish stop words\n",
    "    'classifier__n_estimators': [50, 100],  # Reduce number of combinations\n",
    "    'classifier__learning_rate': [0.1, 0.01]\n",
    "}\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('classifier', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, verbose=1, n_jobs=-1, error_score='raise')\n",
    "try:\n",
    "    grid_search.fit(X_train, y_train)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during grid search: {e}\")\n",
    "\n",
    "# Check if grid search was successful\n",
    "if hasattr(grid_search, 'best_estimator_'):\n",
    "    # Get best hyperparameters and best score\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    # Print best hyperparameters and best score\n",
    "    print(\"Best hyperparameters:\", best_params)\n",
    "    print(\"Best cross-validation score:\", best_score)\n",
    "\n",
    "    # Evaluate the best model on a holdout test set\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    test_score = accuracy_score(y_test, y_pred)\n",
    "    print(\"Test score of the best model:\", test_score)\n",
    "\n",
    "    # Print the classification report\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "else:\n",
    "    print(\"Grid search did not complete successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
